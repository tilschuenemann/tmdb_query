
# program options ---------------------------------------------------------

CHECK_FOR_OLD_DB <- FALSE
if (CHECK_FOR_OLD_DB==F){
  DB_EXISTS = F
}

CONSTANT_WD <- "/mnt/4TB/movies_kodi/"

# start benchmark
start_time <- Sys.time()

# check if packages are installed and load --------------------------------
packages = c("httr", "jsonlite",
             "dplyr", "tidyr","stringr",
             "tcltk")


package.check <- lapply(
  packages,
  FUN = function(x) {
    if (!require(x, character.only = TRUE)) {
      install.packages(x, dependencies = TRUE)
      library(x, character.only = TRUE)
    }
  }
)

remove(packages, package.check)

# open dir and check for contents -----------------------------------------

if(is.null(CONSTANT_WD)==T){
# open dialog for setting wd
# using tcltk as other options differ by os
setwd(tclvalue(tkchooseDirectory()))
}
# is dir empty?
disc_df <- as.data.frame(list.dirs(full.names = F,recursive = F))
names(disc_df) <- "disc_dirname"

if(nrow(disc_df)==0){
  print("dir is empty")
  stop()
}



# read contents -----------------------------------------------------------

disc_df_extr <- disc_df %>%
  # TODO remove brackets from subtitle extraction
  # TODO year should extract from second last position, also extract without literal brackets
  mutate(cache_subs = str_extract(string = disc_dirname,pattern = "\\([:alpha:]*\\)$"),
         cache_year = str_extract(string = disc_dirname, pattern = "\\([:digit:]{4,}\\)")) %>%
  mutate(disc_title = substr(disc_dirname,
                              0,
                              str_length(disc_dirname)-(str_length(cache_subs)+str_length(cache_year))-2),
         cache_subs = gsub("\\(|\\)", "", cache_subs),
         cache_year = gsub("\\(|\\)", "", cache_year)) %>%
  mutate(cache_urltitle = gsub("[ ]", "+", disc_title)) # prepare for url

print(paste0(nrow(disc_df_extr)," movies found in dir"))

remove(disc_df)

# check for api key -------------------------------------------------------

if(file.exists("tmdb_query_apikey.csv")==T){
  # read api key
  api_key <- read.csv2("tmdb_query_apikey.csv")[,2]

  } else {
  print("no existing config found")
  
  # create and write new csv for api key
  tmdb_query_config <- as.data.frame("api_key")
  names(tmdb_query_config) <- "generated by tmdb_query"
  tmdb_query_config$value <- NA
  write.csv2(tmdb_query_config, "tmdb_query_apikey.csv", row.names = F)
  
  # os specific 
  filesep = .Platform$file.sep
  config_fp <- paste0("created ",getwd(),filesep,"tmdb_query_apikey.csv")
  print(config_fp)
}

# rough check for valid key -----------------------------------------------
if(str_length(api_key)!=32){
  print("the provided api_key seems to be incorrect")
  stop()
}

# check and read existing cache -------------------------------------------
if(CHECK_FOR_OLD_DB==T){
  
old_db <- "tmdb_db.csv"
DB_EXISTS <- file.exists(old_db)

if(DB_EXISTS==T){
  # read cache
  old_db <- read.csv2(old_db)

  old_dir_df <- old_db %>%
    select("disc_title", "disc_dirname")

  
  new_dir_df <- disc_df_extr %>%
    select("disc_title", "disc_dirname")

  str(old_dir_df)
  str(new_dir_df)

  old_dir_df %>%
    distinct() %>%
    nrow()

  new_dir_df %>%
    distinct() %>%
    nrow()

  setdiff(new_dir_df,old_dir_df) # these exist in actual dir but not in old db
  setdiff(old_dir_df,new_dir_df) # these exist in old db but not in actual dir

  semi_join(old_dir_df, new_dir_df, by = "disc_dirname")
  anti_join(new_dir_df,old_dir_df , by = "disc_dirname")
  anti_join(old_dir_df, new_dir_df, by = "disc_dirname")
  semi_join(new_dir_df,old_dir_df , by = "disc_dirname")

  # get movies with no tmdb info
  noinfo_df <- old_db %>%
    filter(is.na(tmdb_title))
  
  # get disc extr data for no info movies
  disc_df_extr <- semi_join(disc_df_extr, noinfo_df, by ="disc_dirname") 
  
}

}
#else {
 # print("no existing cache found in this working directory")
  #print("building a new one")
#}

# tmdb api calls ----------------------------------------------------------

tmdb_list <- NULL
movies_not_found <- NULL
j <- nrow(disc_df_extr)

for (i in 1:j) {
  #print(i)
  
  # query with title and year of movie
  title = disc_df_extr$cache_urltitle[i]
  year = disc_df_extr$cache_year[i]
  
  # prepare call with title and year, another call only with title as backup
  url1 = paste0("https://api.themoviedb.org/3/search/movie?api_key=",api_key,"&query=",title,"&year=",year)
  url2 = paste0("https://api.themoviedb.org/3/search/movie?api_key=",api_key,"&query=",title)
  
  unpack_query <- function(url_arg){
    
    res = GET(url_arg)
    
    # convert from raw characters to json to dataframe
    single_call <- res$content %>%
      rawToChar() %>%
      fromJSON()
    
    # unlist and format
    single_call<-unlist(single_call[2],recursive = F, use.names = T) %>%
      do.call(cbind,.) %>%
      as.data.frame()
    
    # change header prefix
    names(single_call) <- str_replace(names(single_call), pattern = "results.", replacement = "tmdb_")
    
    return(single_call)
  } 
  
  single_call <- unpack_query(url1)
  
  # check without year if there are no results
  if(nrow(single_call)==0){
    single_call <- unpack_query(url2)
  }
  
  # get case for pushing result to final list
  n <- nrow(single_call)
  
  if(n==0 | is.null(n)){
    print(paste0("No results found for: ",disc_df_extr$disc_title[i],", year: ",year, ", id: ",i))
    
    movies_not_found <- rbind(disc_df_extr[i,c("disc_title","disc_dirname")],movies_not_found)
    next
    
  } else if(n>= 1){
    
    # get most popular if there are more results
    if(n > 1){
      single_call <- single_call %>%
        slice(which.max(tmdb_popularity))
    }
    
    # add references for comparison later
    single_call$disc_title = unlist(disc_df_extr$disc_title[i])
    single_call$disc_dirname = disc_df_extr$disc_dirname[i]
    
    # TODO unnest everything except genre ids
    single_call <-  unnest(single_call,cols = c(names(single_call)), keep_empty = T)
   # str(single_call)
    
    
    tmdb_list <- rbind(single_call, tmdb_list)
  } #end if
  
}

remove(n, i, j, url1, url2, title, year)

# merge calls
if(is.null(movies_not_found)!=T && is.null(tmdb_list)!=T){
  new_db <- merge(tmdb_list, movies_not_found, by = c("disc_title","disc_dirname"),all = T) 
  
} else if (is.null(movies_not_found)){
  new_db <- tmdb_list
} else if (is.null(tmdb_list)){
  new_db <- movies_not_found
}

# keep distinct
new_db <- new_db %>%
  distinct(disc_dirname, .keep_all = T)

# unnest
new_db <- unnest(new_db,keep_empty = T)

if(DB_EXISTS){
  # keep cache and add new movies
  new_db <- merge(new_db, old_db, by = c("disc_title","disc_dirname"),all = T)

}

# order columns by name
new_db <- new_db[, order(names(new_db))]

# write to disc
write.csv2(new_db, "tmdb_db.csv", row.names = F)

# end benchmark
end_time <- Sys.time()
print(end_time - start_time)
